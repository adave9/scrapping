{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests                # To handle http or https requests\n",
    "from bs4 import BeautifulSoup  # In order to pull html or xml files\n",
    "import urllib3                 # Url library handler to apply various functions on a url\n",
    "import pandas                  # Library used for data manipulation and analysis\n",
    "from lxml import html          # lxml comes with a dedicated Python package for dealing with HTML\n",
    "import unicodecsv as csv       # Importing unicode to csv library\n",
    "import argparse                # Argument parsing library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare a headers variable to include metada tags\n",
    "headers= {\n",
    "            'accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'accept-encoding':'gzip, deflate, sdch, br',\n",
    "            'accept-language':'en-GB,en;q=0.8,en-US;q=0.6,ml;q=0.4',\n",
    "            'cache-control':'max-age=0',\n",
    "            'upgrade-insecure-requests':'1',\n",
    "            'user-agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'\n",
    "         }\n",
    "# Creating a variable to pass the url\n",
    "url = \"https://www.apartments.com/chicago-il/\"\n",
    "# Use function get within requests library to get contents from url\n",
    "r = requests.get(url,headers=headers)\n",
    "#  To create a variable c and handle contents recieved from url\n",
    "c = r.content\n",
    "# Use BeautifulSoup library to parse the contents\n",
    "soup = BeautifulSoup(c,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to scan the pages on website declare two variables ffor first and last page\n",
    "start_page = 1\n",
    "last_page = 50\n",
    "# Create an array to list those contents\n",
    "web_content_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a for loop to scan from first to last page\n",
    "for page_number in range(int(start_page),int(last_page)):\n",
    "            # Final_url handles all html webpage one by one between range\n",
    "            # To form the url based on page numbers\n",
    "            final_url = url+str(page_number)+\"/.html\"\n",
    "            # To extract the Title and the Location\n",
    "            placard_header = soup.find_all(\"header\",{\"class\":\"placardHeader\"})\n",
    "            # To extract the Rent, No of Beds and Phone Number\n",
    "            placard_content = soup.find_all(\"section\",{\"class\" :\"placardContent\"})\n",
    "            # To process property by property by looping\n",
    "            for item_header,item_content in zip(placard_header,placard_content):\n",
    "              # To store the information to a dictionary we created earlier\n",
    "              web_content_dict = {}\n",
    "              web_content_dict[\"Title\"]=item_header.find(\"a\",{\"class\":\"placardTitle js-placardTitle\"}).text\n",
    "              web_content_dict[\"Address\"] = item_header.find(\"div\",{\"class\":\"location\"}).text\n",
    "              web_content_dict[\"Price\"] = item_content.find(\"span\",{\"class\":\"altRentDisplay\"}).text\n",
    "              web_content_dict[\"Beds\"] = item_content.find(\"span\",{\"class\":\"unitLabel\"}).text\n",
    "              web_content_dict[\"Phone\"] = item_content.find(\"div\",{\"class\":\"phone\"}).find(\"span\").text\n",
    "              # To store the dictionary to into a list\n",
    "              web_content_list.append(web_content_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make a dataframe with the list\n",
    "df = pandas.DataFrame(web_content_list)\n",
    "# To write the dataframe to a csv file\n",
    "df.to_csv(\"Output_chicago_il_assignment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
